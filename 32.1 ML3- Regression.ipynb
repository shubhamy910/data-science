{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b760f933-6186-4694-bf84-6718aa9cfadc",
   "metadata": {},
   "source": [
    "## Q1. Explain the difference between simple linear regression and multiple linear regression. Provide an example of each."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc394a84-462b-4a56-8a6a-00a9491a9e9b",
   "metadata": {},
   "source": [
    "# Answer1-\n",
    "Simple Linear Regression:\n",
    "\n",
    "- Simple linear regression is a statistical method used to model the relationship between a single independent variable (predictor) and a dependent variable (outcome).\n",
    "- The model is represented as Y = b0 + b1*X + ε, where Y is the dependent variable, X is the independent variable, b0 is the intercept, b1 is the slope coefficient, and ε is the error term.\n",
    "\n",
    "Example: Predicting a student's final exam score (Y) based on the number of hours they studied (X).\n",
    "\n",
    "Multiple Linear Regression:\n",
    "\n",
    "- Multiple linear regression extends simple linear regression to model the relationship between a dependent variable and multiple independent variables.\n",
    "- The model is represented as Y = b0 + b1X1 + b2X2 + ... + bn*Xn + ε, where Y is the dependent variable, X1, X2, ..., Xn are the independent variables, b0 is the intercept, and b1, b2, ..., bn are the coefficients for the respective independent variables.\n",
    "\n",
    "Example: Predicting a house's price (Y) based on multiple features such as square footage (X1), number of bedrooms (X2), and neighborhood quality (X3)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dff6b9d-c6c5-4aba-91de-ba2102958be7",
   "metadata": {},
   "source": [
    "## Q2. Discuss the assumptions of linear regression. How can you check whether these assumptions hold in a given dataset?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d740832-6688-4957-add0-b2cb86902dd9",
   "metadata": {},
   "source": [
    "## Answer2-\n",
    "Assumptions of Linear Regression:\n",
    "\n",
    "- Linearity: The relationship between the dependent and independent variables is linear.\n",
    "- Independence: Residuals (error terms) are independent of each other.\n",
    "- Homoscedasticity: The variance of residuals is constant across all levels of the independent variables.\n",
    "- Normality: Residuals are normally distributed.\n",
    "\n",
    "You can check these assumptions by:\n",
    "\n",
    "- Plotting residuals vs. predicted values to check for linearity and homoscedasticity.\n",
    "- Plotting a histogram or Q-Q plot of residuals to assess normality.\n",
    "- Using statistical tests like the Durbin-Watson test for autocorrelation to check for independence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18171eb2-d6f4-49fb-ad77-9d17f05458ef",
   "metadata": {},
   "source": [
    "# Q3. How do you interpret the slope and intercept in a linear regression model? Provide an example using a real-world scenario."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5004a125-c7b5-445f-8add-69a05d75355b",
   "metadata": {},
   "source": [
    "# Answer3-\n",
    "Slope (b1): It represents the change in the dependent variable (Y) for a one-unit change in the independent variable (X), holding all other variables constant.\n",
    "\n",
    "Intercept (b0): It represents the predicted value of the dependent variable (Y) when all independent variables are set to zero.\n",
    "\n",
    "Example: In a salary prediction model, if the slope for years of experience (X) is 2.5, it means that, on average, each additional year of experience is associated with a $2,500 increase in salary. The intercept represents the starting salary for someone with zero years of experience.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53bfe3a8-9b9e-44fc-9c8d-9cf303a9176d",
   "metadata": {},
   "source": [
    "# Q4. Explain the concept of gradient descent. How is it used in machine learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3fb4eda-48a9-4d7f-9b54-ee24163855dd",
   "metadata": {},
   "source": [
    "## Answer4-\n",
    "- Gradient descent is an optimization algorithm used in machine learning to minimize the cost function (error) of a model by adjusting its parameters iteratively.\n",
    "- It works by calculating the gradient (derivative) of the cost function with respect to the model parameters and updating the parameters in the opposite direction of the gradient to reach the minimum cost.\n",
    "- It is used to train various machine learning models, including linear regression, neural networks, and support vector machines, by finding the optimal parameter values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f9c5a83-d8ed-47e3-b516-779b4b823cb0",
   "metadata": {},
   "source": [
    "# Q5. Describe the multiple linear regression model. How does it differ from simple linear regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ccf7567-eab4-404f-8cfe-c6c0e271e76f",
   "metadata": {},
   "source": [
    "# Answer5-\n",
    "Multiple Linear Regression:\n",
    "\n",
    "- Multiple linear regression is an extension of simple linear regression that models the relationship between a dependent variable and multiple independent variables.\n",
    "- It allows for a more complex modeling of how multiple independent variables collectively influence the dependent variable.\n",
    "- The model is represented as Y = b0 + b1X1 + b2X2 + ... + bn*Xn + ε, where Y is the dependent variable, and X1, X2, ..., Xn are the independent variables.\n",
    "\n",
    "Difference from Simple Linear Regression:\n",
    "\n",
    "- Simple linear regression has only one independent variable, whereas multiple linear regression has multiple independent variables.\n",
    "- Multiple linear regression provides a more comprehensive understanding of the relationship between the dependent variable and multiple predictors.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cdf7a97-2719-4d2a-a41d-33336bbae985",
   "metadata": {},
   "source": [
    "# Q6. Explain the concept of multicollinearity in multiple linear regression. How can you detect and address this issue?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc4ebe62-e029-4e1c-aa7b-20e056649a2b",
   "metadata": {},
   "source": [
    "## Answer6-\n",
    "- Multicollinearity refers to the high correlation between two or more independent variables in a multiple linear regression model.\n",
    "\n",
    "- It can make it challenging to determine the individual effect of each independent variable on the dependent variable.\n",
    "\n",
    "- To detect multicollinearity, you can calculate correlation coefficients between independent variables or use variance inflation factors (VIFs).\n",
    "\n",
    "- To address multicollinearity, you can remove one of the correlated variables, combine them into a single variable, or use regularization techniques like ridge regression."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98b2e69f-5e4a-4df5-a3e1-2c07010c4709",
   "metadata": {},
   "source": [
    "# Q7. Describe the polynomial regression model. How is it different from linear regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cf36b8e-341e-46ac-b255-2c7cefe38207",
   "metadata": {},
   "source": [
    "## Answer7-\n",
    "\n",
    "- Polynomial regression is a form of regression analysis where the relationship between the dependent variable and the independent variable is modeled as an nth-degree polynomial.\n",
    "- It allows for more flexible modeling of non-linear relationships between variables.\n",
    "- The model equation is Y = b0 + b1X + b2X^2 + ... + bn*X^n + ε.\n",
    "\n",
    "Difference from Linear Regression:\n",
    "\n",
    "- Linear regression models relationships as linear (straight lines), while polynomial regression models them as polynomial curves.\n",
    "- Polynomial regression can capture more complex, non-linear patterns in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26731200-477b-4a67-af83-c627294ce324",
   "metadata": {},
   "source": [
    "# Q8. What are the advantages and disadvantages of polynomial regression compared to linear regression? In what situations would you prefer to use polynomial regression?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e377daaa-5df6-4dfa-9d47-1390456f0481",
   "metadata": {},
   "source": [
    "## Answer8-\n",
    "Advantages:\n",
    "\n",
    "- Can model non-linear relationships.\n",
    "- Offers flexibility in fitting various types of curves.\n",
    "- Can be useful when the true relationship between variables is not known.\n",
    "\n",
    "Disadvantages:\n",
    "\n",
    "- Prone to overfitting with high-degree polynomials.\n",
    "- Can become computationally expensive with a large number of features.\n",
    "- Interpretability can be challenging as the model becomes more complex.\n",
    "\n",
    "Use Cases:\n",
    "\n",
    "- Use polynomial regression when you suspect a non-linear relationship between variables or when linear regression fails to capture the underlying pattern. However, use it cautiously to avoid overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f8d96a-e710-440f-9fe4-2a2d577957eb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
