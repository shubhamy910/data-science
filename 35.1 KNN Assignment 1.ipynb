{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "811336da-afc0-448e-800e-8bc23124af98",
   "metadata": {},
   "source": [
    "Q1. What is the KNN algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a6556f8-36c7-4d47-926d-e5e46df3e792",
   "metadata": {},
   "source": [
    "Answer1. KNN stands for \"k-nearest neighbors.\" It's a simple and versatile machine learning algorithm used for both classification and regression tasks. In KNN, an object is classified by a majority vote of its k nearest neighbors. It works based on the principle that objects that are similar are likely to be in the same class or have similar values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39ccd2f4-699d-4f28-9d26-e127bdff2192",
   "metadata": {},
   "source": [
    "Q2. How do you choose the value of K in KNN?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f70f6b8-73a6-4337-a391-75576aab3b2e",
   "metadata": {},
   "source": [
    "Answer2. The choice of the value of k in KNN is crucial and can significantly impact the model's performance. A smaller value of k (e.g., 1 or 3) can make the model sensitive to noise, while a larger value of k can make the model overly biased. To choose an appropriate value for k, you can use techniques like cross-validation or grid search to find the value that yields the best performance on your dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa53d8ca-50d6-474f-a076-787141e9eb24",
   "metadata": {},
   "source": [
    "Q3. What is the difference between KNN classifier and KNN regressor?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fc1a32c-cb7b-4946-b757-93f4cb63b608",
   "metadata": {},
   "source": [
    "Answer3. \n",
    "- KNN Classifier: In KNN classification, the algorithm predicts the class label of a data point based on the majority class among its k nearest neighbors.\n",
    "- KNN Regressor: In KNN regression, the algorithm predicts a continuous value (e.g., a numerical quantity) for a data point based on the average or weighted average of the values of its k nearest neighbors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e736e19-bd04-49b2-ab40-5f3e8310e4d6",
   "metadata": {},
   "source": [
    "Q4. How do you measure the performance of KNN?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7aaf4f4-bd91-4c53-86b0-19b9014b081e",
   "metadata": {},
   "source": [
    "Answer4. The performance of a KNN model can be measured using various metrics depending on whether it's a classification or regression problem. Common metrics include accuracy, precision, recall, F1-score, Mean Absolute Error (MAE), Mean Squared Error (MSE), and R-squared (R^2), among others. The choice of metric depends on the specific problem and goals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2caa6257-27c3-42a0-9e97-31d917a571d4",
   "metadata": {},
   "source": [
    "Q5. What is the curse of dimensionality in KNN?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "210f6b70-be09-44d7-a62a-2ddd16a80335",
   "metadata": {},
   "source": [
    "Answer5. The curse of dimensionality refers to the problem that arises when working with high-dimensional data in KNN. As the number of dimensions (features) increases, the distance between data points becomes less meaningful, and KNN may perform poorly. This is because points in high-dimensional space tend to be far apart, and the nearest neighbors may not be truly representative. Techniques such as dimensionality reduction or feature selection can help mitigate this issue."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2609321-b4a1-40be-b9bb-cf6d3ef67ba2",
   "metadata": {},
   "source": [
    "Q6. How do you handle missing values in KNN?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8575dfb1-d015-4173-a3d5-a0a4055fee43",
   "metadata": {},
   "source": [
    "Answer6. Handling missing values in KNN can be challenging. You can either impute missing values before applying KNN or modify the distance metric to handle missing values during the computation of distances. Common imputation methods include mean imputation, median imputation, or using more advanced techniques like k-nearest neighbor imputation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1fd5757-6177-408a-9aa5-aee11691105a",
   "metadata": {},
   "source": [
    "Q7. Compare and contrast the performance of the KNN classifier and regressor. Which one is better for which type of problem?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b951e10-659a-4818-b6c4-7af232039b2e",
   "metadata": {},
   "source": [
    "Answer7.\n",
    "\n",
    "- KNN Classifier is suitable for problems where the goal is to classify data into discrete classes or categories (e.g., spam detection, image classification).\n",
    "- KNN Regressor is suitable for problems where the goal is to predict continuous numerical values (e.g., predicting house prices, stock prices).\n",
    "- The choice between classifier and regressor depends on the nature of the problem and the type of output you need.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "826f81eb-da52-4886-845f-d6362841ba8d",
   "metadata": {},
   "source": [
    "Q8. What are the strengths and weaknesses of the KNN algorithm for classification and regression tasks, and how can these be addressed?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf2c6138-4efe-40d4-a6d6-0abef636b7f6",
   "metadata": {},
   "source": [
    "Answer8.\n",
    "\n",
    "Strengths:\n",
    "\n",
    "- Simple to understand and implement.\n",
    "- Can handle both classification and regression tasks.\n",
    "- Non-parametric (no assumptions about data distribution).\n",
    "\n",
    "Weaknesses:\n",
    "\n",
    "- Sensitive to the choice of k.\n",
    "- Computationally expensive for large datasets.\n",
    "- Suffers from the curse of dimensionality.\n",
    "\n",
    "Addressing weaknesses can involve careful feature selection, dimensionality reduction, and optimizing the choice of k."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee657be8-2e1d-4cfc-9f03-c70f49d570ee",
   "metadata": {},
   "source": [
    "Q9. What is the difference between Euclidean distance and Manhattan distance in KNN?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4670580e-c02e-4ab4-aaf6-8c5923c4e0f4",
   "metadata": {},
   "source": [
    "Answer9.\n",
    "\n",
    "- Euclidean Distance: It measures the \"as-the-crow-flies\" or straight-line distance between two points in Euclidean space. In KNN, it is often used to measure the distance between data points.\n",
    "- Manhattan Distance: It measures the distance between two points as the sum of the absolute differences of their coordinates. In KNN, it's used as an alternative distance metric when the data is not naturally represented in Euclidean space."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e1dbf6-ab46-4c19-abee-955b5e06837a",
   "metadata": {},
   "source": [
    "Q10. What is the role of feature scaling in KNN?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb044329-f54f-483c-b14d-06ba2ffb4ac5",
   "metadata": {},
   "source": [
    "Answer10. Feature scaling is important in KNN because the algorithm relies on distance metrics to find the nearest neighbors. If features have different scales, features with larger scales can dominate the distance calculation. Standardizing or normalizing features (e.g., using z-score normalization or min-max scaling) ensures that all features contribute equally to the distance calculation and improves the performance of KNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba623278-81f6-4ba5-9f8e-582f54a56bb5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
