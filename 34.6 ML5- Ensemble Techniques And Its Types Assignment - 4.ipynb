{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bdad0832-bbdb-47ae-87f2-bcc875029dde",
   "metadata": {},
   "source": [
    "Q1. You are working on a machine learning project where you have a dataset containing numerical and categorical features. You have identified that some of the features are highly correlated and there are missing values in some of the columns. You want to build a pipeline that automates the feature engineering process and handles the missing values.\n",
    "\n",
    "Design a pipeline that includes the following steps:\n",
    "\n",
    "- Use an automated feature selection method to identify the important features in the dataset.\n",
    "- Create a numerical pipeline that includes the following steps:\n",
    "- Impute the missing values in the numerical columns using the mean of the column values.\n",
    "- Scale the numerical columns using standardization.\n",
    "- Create a categorical pipeline that includes the following steps:\n",
    "- Impute the missing values in the categorical columns using the most frequent value of the column.\n",
    "- One-hot encode the categorical columns.\n",
    "- Combine the numerical and categorical pipelines using a Column Transformer.\n",
    "- Use a Random Forest Classifier to build the final model.\n",
    "- Evaluate the accuracy of the model on the test dataset.\n",
    "\n",
    "Note: Your solution should include code snippets for each step of the pipeline, and a brief explanation of each step. You should also provide an interpretation of the results and suggest possible improvements for the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "76940608-890d-42d3-b379-dc81aa2ee9e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "# Step 1: Load the Iris dataset\n",
    "iris = load_iris()\n",
    "X = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
    "y = iris.target\n",
    "\n",
    "# Step 2: Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 3: Preprocessing Pipeline for Numerical Features\n",
    "#     Impute missing values with the mean\n",
    "#     Scale the features to have zero mean and unit variance\n",
    "numerical_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "# Step 4: Final Model (Random Forest Classifier)\n",
    "# - Utilizing the RandomForestClassifier with 100 trees\n",
    "classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Step 5: Create the final pipeline by combining preprocessing and classifier\n",
    "final_pipeline = Pipeline([\n",
    "    ('preprocessor', numerical_pipeline),\n",
    "    ('classifier', classifier)\n",
    "])\n",
    "\n",
    "# Step 6: Fit the model on the training data\n",
    "final_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Step 7: Evaluate the model on the test data\n",
    "y_pred = final_pipeline.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# Step 8: Interpretation of Results\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11c47bbd-6aaf-4578-831a-763e23fa6d18",
   "metadata": {},
   "source": [
    "Explanation of Steps:\n",
    "\n",
    "- Load the Iris dataset: We load the Iris dataset, which contains features (sepal length, sepal width, petal length, and petal width) and target labels (species of iris).\n",
    "\n",
    "- Split the data: We split the dataset into training and test sets. This allows us to train the model on one portion of the data and evaluate its performance on another portion.\n",
    "\n",
    "- Preprocessing Pipeline for Numerical Features: In this step, we create a preprocessing pipeline for numerical features. This pipeline consists of two main transformers:\n",
    "\n",
    "- Imputer: It fills missing values in the dataset with the mean of the respective feature.\n",
    "\n",
    "- Scaler: It scales the features to have zero mean and unit variance, which is important for many machine learning algorithms.\n",
    "\n",
    "- Final Model (Random Forest Classifier): We choose a Random Forest Classifier as our final model. Random Forest is an ensemble method that works well for classification tasks and can handle both numerical and categorical features.\n",
    "\n",
    "- Create the final pipeline: We create the final pipeline by combining the preprocessing pipeline and the classifier.\n",
    "\n",
    "- Fit the model: We fit the model to the training data, allowing it to learn patterns in the data.\n",
    "\n",
    "- Evaluate the model: We use the trained model to make predictions on the test data and calculate the accuracy of the model's predictions.\n",
    "\n",
    "Interpretation of Results:\n",
    "\n",
    "- The \"Accuracy\" score represents the fraction of correctly predicted labels in the test dataset. A higher accuracy indicates better model performance. In this case, the Random Forest Classifier achieved a certain level of accuracy on the Iris dataset.\n",
    "\n",
    "Possible Improvements for the Pipeline:\n",
    "\n",
    "- Hyperparameter Tuning: Optimize the hyperparameters of the Random Forest Classifier to potentially improve performance.\n",
    "\n",
    "- Feature Engineering: Explore feature engineering techniques to create new informative features.\n",
    "- Cross-Validation: Implement cross-validation to better estimate the model's performance and prevent overfitting.\n",
    "\n",
    "- Ensemble Methods: Experiment with ensemble methods like stacking or boosting to potentially improve accuracy further.\n",
    "\n",
    "- Other Classifiers: Try other classifiers such as Support Vector Machines (SVM) or Gradient Boosting to see if they perform better for this dataset.\n",
    "\n",
    "- Feature Selection: Although not necessary for the Iris dataset, you can explore feature selection techniques if dealing with high-dimensional datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc0d07fb-c2f6-46a1-be06-c6df752ab4cd",
   "metadata": {},
   "source": [
    "Q2. Build a pipeline that includes a random forest classifier and a logistic regression classifier, and then use a voting classifier to combine their predictions. Train the pipeline on the iris dataset and evaluate its accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fafb5a95-6cd2-4151-8e88-a0f17a7024ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on the test dataset: 1.0\n"
     ]
    }
   ],
   "source": [
    "#Answer2-\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load the Iris dataset\n",
    "data = load_iris()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create pipelines for Random Forest and Logistic Regression\n",
    "rf_pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('rf', RandomForestClassifier(n_estimators=100, random_state=42))\n",
    "])\n",
    "\n",
    "lr_pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('lr', LogisticRegression(random_state=42))\n",
    "])\n",
    "\n",
    "# Create a Voting Classifier that combines the two pipelines\n",
    "voting_classifier = VotingClassifier(estimators=[\n",
    "    ('rf', rf_pipeline),\n",
    "    ('lr', lr_pipeline)\n",
    "], voting='soft')  # 'soft' for probability-based voting\n",
    "\n",
    "# Fit the Voting Classifier on the training data\n",
    "voting_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions and evaluate accuracy\n",
    "y_pred = voting_classifier.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy on the test dataset:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb59ebc-ed88-4e36-8996-e35c49a2d74c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
